{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasper-zheng/streamable-stable-audio-open/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qk7Q7YUvi1Tz",
      "metadata": {
        "id": "Qk7Q7YUvi1Tz"
      },
      "source": [
        "# Streaming Stable Audio Open 1.0's Autoencoder  \n",
        "\n",
        "Streaming pre-trained [Stable Audio Open 1.0](https://huggingface.co/stabilityai/stable-audio-open-1.0)'s autoencoder with cached convolution, for realtime continuous inference. And scripting it to TorchScript to be used with [nn~](https://github.com/acids-ircam/nn_tilde) in MaxMSP/PureData.  \n",
        "\n",
        "Author: Jasper Shuoyang Zheng"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AxfvVFuMhh7l",
      "metadata": {
        "id": "AxfvVFuMhh7l"
      },
      "source": [
        "## Installation (Only do this once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FJqbPWuzey3H",
      "metadata": {
        "id": "FJqbPWuzey3H"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/jasper-zheng/streamable-stable-audio-open.git\n",
        "%cd streamable-stable-audio-open\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O_jbMOX3hpdV",
      "metadata": {
        "id": "O_jbMOX3hpdV"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42991dba",
      "metadata": {
        "id": "42991dba"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "base_dir = 'streamable-stable-audio-open'\n",
        "sys.path.append(f'{base_dir}')\n",
        "\n",
        "import torch\n",
        "from models import get_pretrained_pretransform\n",
        "from export import remove_parametrizations\n",
        "torch_250 = True if torch.__version__ >= \"2.5\" else False\n",
        "\n",
        "import librosa, time\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "import cached_conv as cc\n",
        "\n",
        "cc.use_cached_conv(True)\n",
        "\n",
        "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}, torch {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OHSJCPYngzoP",
      "metadata": {
        "id": "OHSJCPYngzoP"
      },
      "source": [
        "## Download and load pre-trained model\n",
        "\n",
        "Before proceed to model downloading, you need to:  \n",
        "1. use `hf auth login` in terminal to login to your HuggingFace account,\n",
        "2. go to [stable-audio-open-1.0](https://huggingface.co/stabilityai/stable-audio-open-1.0) and agree to Stability AI's License Agreement to get access to the model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cb03a29",
      "metadata": {
        "id": "6cb03a29"
      },
      "outputs": [],
      "source": [
        "## Load the autoencoder from stable-audio-open-1.0\n",
        "\n",
        "autoencoder, model_config = get_pretrained_pretransform(\"stabilityai/stable-audio-open-1.0\",\n",
        "                                                         model_half=False,\n",
        "                                                         skip_bottleneck=True,\n",
        "                                                         device=device)\n",
        "\n",
        "print(f\"sample_rate: {model_config.get('sample_rate', 'unknown')}\")\n",
        "print(f\"latent_dim: {model_config['model']['pretransform']['config'].get('latent_dim', 'unknown')}\")\n",
        "print(f\"downsampling_ratio: {model_config['model']['pretransform']['config'].get('downsampling_ratio', 'unknown')}\")\n",
        "print(f\"io_channels: {model_config['model']['pretransform']['config'].get('io_channels', 'unknown')}\")\n",
        "\n",
        "remove_parametrizations(autoencoder)\n",
        "\n",
        "autoencoder = autoencoder.to(device)\n",
        "autoencoder = autoencoder.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FsufRjPSh684",
      "metadata": {
        "id": "FsufRjPSh684"
      },
      "source": [
        "## Prepare audio chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50129db2",
      "metadata": {
        "id": "50129db2"
      },
      "outputs": [],
      "source": [
        "# Load an example audio file and split into chunks\n",
        "\n",
        "buffer_size = 4096\n",
        "\n",
        "audio_path = librosa.example('fishin', hq=True)\n",
        "wv, sr = librosa.load(audio_path, sr=44100, mono=False)\n",
        "wv = torch.tensor(wv, device=device)[:,buffer_size*50:buffer_size*150].unsqueeze(0)  # make stereo, limit length for test\n",
        "wv_chunks = [wv[:, :, i*buffer_size:(i+1)*buffer_size] for i in range(100)]\n",
        "\n",
        "print(f'waveform shape: {wv.shape}')\n",
        "print(f'number of chunks: {len(wv_chunks)}')\n",
        "print(f'chunk shape: {wv_chunks[0].shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pBQmRNjKiCk4",
      "metadata": {
        "id": "pBQmRNjKiCk4"
      },
      "source": [
        "## Forward pass the encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a720383",
      "metadata": {
        "id": "2a720383"
      },
      "outputs": [],
      "source": [
        "print(f'Running encoder, device: {device}')\n",
        "## Run audio chunks to the encoder\n",
        "\n",
        "latent_chunks = []\n",
        "with torch.no_grad():\n",
        "    if torch_250:\n",
        "        torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "    for i, w in enumerate(wv_chunks):\n",
        "        latent = autoencoder.encode(w)\n",
        "        latent_chunks.append(latent)\n",
        "\n",
        "    if torch_250:\n",
        "        torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
        "        print(f'Encoder execution time: {time.perf_counter() - start_time:.2f} seconds')\n",
        "\n",
        "\n",
        "print(f'Running decoder, device: {device}')\n",
        "## Run audio chunks to the decoder\n",
        "wv_recons = []\n",
        "with torch.no_grad():\n",
        "    if torch_250:\n",
        "        torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "    for i, latent in enumerate(latent_chunks):\n",
        "        wv_recon = autoencoder.decode(latent)\n",
        "        wv_recons.append(wv_recon)\n",
        "\n",
        "    if torch_250:\n",
        "        torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
        "        print(f'Decoder execution time: {time.perf_counter() - start_time:.2f} seconds')\n",
        "\n",
        "wv_recon = torch.cat(wv_recons, dim=-1)\n",
        "print(f'reconstructed waveform shape: {wv_recon.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62700ac9",
      "metadata": {
        "id": "62700ac9"
      },
      "outputs": [],
      "source": [
        "\"Original:\"\n",
        "display(Audio(wv.cpu().numpy()[0], rate=sr))\n",
        "\"Reconstructed:\"\n",
        "display(Audio(wv_recon.cpu().numpy()[0], rate=sr))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jXJPzGygiRKt",
      "metadata": {
        "id": "jXJPzGygiRKt"
      },
      "source": [
        "## Export to TorchScript  \n",
        "\n",
        "If you have [nn~](https://github.com/acids-ircam/nn_tilde) in MaxMSP/PureData:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WbCqXEa1iUi5",
      "metadata": {
        "id": "WbCqXEa1iUi5"
      },
      "outputs": [],
      "source": [
        "!python streamable-stable-audio-open/export.py --output exported/stable-ae-float32-torch25x.ts --streaming\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44e9b9e2",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "stableaudio",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
